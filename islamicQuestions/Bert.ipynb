{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from gensim.models import FastText\n",
    "import tensorflow as tf\n",
    "import stanza\n",
    "import numpy as np\n",
    "import openai\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Answers islamic - Feuille 1.csv\")\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zakar\\OneDrive\\Bureau\\islamicProject\\zakarenv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "bert_model = TFBertModel.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02')\n",
    "transformer_model = load_model(\"Transformer_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # Load GPT-2 tokenizer\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # Load GPT-2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def generate_feedback(bert_output, student_answer, score):\n",
    "    # Encode student answer with BERT tokenizer\n",
    "    bert_input_ids = bert_tokenizer.encode(student_answer, return_tensors='tf')\n",
    "\n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        bert_output = bert_model(bert_input_ids)[0]\n",
    "\n",
    "    # Construct prompt for GPT-2\n",
    "    prompt = f\"Here is the student answer: {student_answer}. Their score was {score}. Based on this information, provide comprehensive and personalized feedback, highlighting strengths and areas for improvement. Be respectful and encouraging.\"\n",
    "\n",
    "    # Encode prompt with GPT-2 tokenizer\n",
    "    gpt2_input_ids = gpt2_tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate feedback with GPT-2\n",
    "    gpt2_output = gpt2_model.generate(gpt2_input_ids, max_length=150, num_beams=5, no_repeat_ngram_size=2)\n",
    "\n",
    "    # Decode GPT-2 output\n",
    "    feedback_text = gpt2_tokenizer.decode(gpt2_output[0], skip_special_tokens=True)\n",
    "\n",
    "    return feedback_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6bbf2850cd4e98862c508cc9975f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 18:21:13 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-01-05 18:21:14 INFO: File exists: C:\\Users\\zakar\\stanza_resources\\ar\\default.zip\n",
      "2024-01-05 18:21:17 INFO: Finished downloading models and saved to C:\\Users\\zakar\\stanza_resources.\n",
      "2024-01-05 18:21:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26fedcd5d674b749649e5019e92ce02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 18:21:19 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-01-05 18:21:19 INFO: Using device: cpu\n",
      "2024-01-05 18:21:19 INFO: Loading: tokenize\n",
      "2024-01-05 18:21:19 INFO: Loading: mwt\n",
      "2024-01-05 18:21:19 INFO: Loading: pos\n",
      "2024-01-05 18:21:19 INFO: Loading: lemma\n",
      "2024-01-05 18:21:19 INFO: Loading: depparse\n",
      "2024-01-05 18:21:20 INFO: Loading: ner\n",
      "2024-01-05 18:21:20 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download(\"ar\")\n",
    "nlp = stanza.Pipeline(\"ar\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        word.lemma\n",
    "        for sent in doc.sentences\n",
    "        for word in sent.words\n",
    "        if word.upos != \"PUNCT\"\n",
    "    ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"answers\"] = data[\"answers\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [عيسى, علي, هُوَ, سَلَام]\n",
       "1         [نَبِيّ, عيسى, عَلَى, هُوَ, سَلَام]\n",
       "2      [نبي, الله, عيسى, عَلَى, هُوَ, سَلَام]\n",
       "3                           [نبي, الله, عيسى]\n",
       "4                              [نَبِيّ, عيسى]\n",
       "                        ...                  \n",
       "972                       [لَا, أعرف, جَوَاب]\n",
       "973                               [لَا, أعرف]\n",
       "974                              [لَا, عَلِم]\n",
       "975                                     [لوط]\n",
       "976                                    [ثمود]\n",
       "Name: answers, Length: 842, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Keras tokenizer\n",
    "tokenizer = Tokenizer(filters=\"\"\"'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ\"\"\"\"\")\n",
    "# Fit the tokenizer on your text data\n",
    "tokenizer.fit_on_texts(data[\"answers\"])\n",
    "# Convert your text data to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(data[\"answers\"])\n",
    "# Find the length of the longest sequence\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "# Pad your sequences so they all have the same length\n",
    "sequences = pad_sequences(sequences, max_sequence_length)\n",
    "# Get a dictionary where the keys are words and the values are their corresponding integer values\n",
    "word2idx = tokenizer.word_index\n",
    "# Get the size of your vocabulary\n",
    "vocab_size = len(word2idx) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of your embeddings\n",
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "# Train FastText model\n",
    "fasttext_model = FastText(data[\"answers\"], vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=4)\n",
    "fasttext_model.save(\"fasttext_model2.bin\")\n",
    "# Fill the embedding matrix with FastText vectors\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if word in fasttext_model.wv.key_to_index:\n",
    "        embedding_matrix[idx] = fasttext_model.wv[word]\n",
    "    else:\n",
    "        print(\"word not exist in voca ---> \" + word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    },
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m bert_output \u001b[38;5;241m=\u001b[39m bert_model(bert_input_ids)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Generate feedback\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m feedback_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_feedback_alpaca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated feedback:\u001b[39m\u001b[38;5;124m\"\u001b[39m, feedback_text)\n",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m, in \u001b[0;36mgenerate_feedback_alpaca\u001b[1;34m(bert_output, student_answer, score)\u001b[0m\n\u001b[0;32m      8\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mHere is the student answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_answer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Their score was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Based on this information, provide comprehensive and personalized feedback, highlighting strengths and areas for improvement. Be respectful and encouraging.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Generate feedback with Alpaca\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-davinci-003\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use Alpaca's text-davinci-003 engine\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust temperature for creativity\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m feedback_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feedback_text\n",
      "File \u001b[1;32mc:\\Users\\zakar\\OneDrive\\Bureau\\islamicProject\\zakarenv\\Lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "student_answer = input(\"Enter student answer: \")\n",
    "student_answer_ids = tokenizer.texts_to_sequences([student_answer])[0]\n",
    "student_answer_ids = pad_sequences([student_answer_ids], maxlen=24)\n",
    "score = transformer_model.predict([student_answer_ids])[0][0]  # Replace with your model's prediction logic\n",
    "# Encode student answer with BERT tokenizer\n",
    "bert_input_ids = bert_tokenizer.encode(student_answer, return_tensors='tf')\n",
    "# Get BERT embeddings\n",
    "bert_output = bert_model(bert_input_ids)[0]\n",
    "# Generate feedback\n",
    "feedback_text = generate_feedback(bert_output, student_answer, score)\n",
    "print(\"Generated feedback:\", feedback_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zakarenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
